{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f537d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e8ebfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f221f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1d0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64d001",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18cbc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name,extension = os.path.splitext(file)\n",
    "    \n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        # each document contains the page, content and metadata with a page number\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "def load_from_wikipedia(query,lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query,lang=lang,load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eadc8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980718bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total tokens: {total_tokens}')\n",
    "    print(f'Embeeding cost in USD: {total_tokens / 1000 * 0.0004:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056833ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_of_fetch_embeddings(index_name):\n",
    "    \n",
    "    import pinecone \n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "    \n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "        \n",
    "    else:\n",
    "        print(f'Creating Index {index_name} and embeddings', end='')\n",
    "        pinecone.create_index(index_name, dimension=1536, metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(chunks,embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6946fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone \n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name=='all':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print('Deleting all the indexes...')\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "        print('ok')\n",
    "    else:\n",
    "        print(f'Deleting inde {index_name}...', end='')\n",
    "        pinecone.delete_index(index)\n",
    "        print('ok')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cadd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo',temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "   \n",
    "    answer = chain.run(q)\n",
    "    \n",
    "    return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8216fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory(vector_store, q, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})\n",
    "    \n",
    "    crc = ConversationalRetrievalChain.from_llm(llm,retriever)\n",
    "    result = crc({'question': q, 'chat_history': chat_history})\n",
    "    \n",
    "    chat_history.append((question, result['answer']))\n",
    "    \n",
    "    return result, chat_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c39fd3",
   "metadata": {},
   "source": [
    "## Running code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3126f08",
   "metadata": {},
   "source": [
    "### Uploading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34168e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/Análisis+de+un+sector+2.0.pdf\n"
     ]
    }
   ],
   "source": [
    "#data = load_document('files/Análisis+de+un+sector+2.0.pdf')\n",
    "#print(data[1].page_content)\n",
    "#print(data[1].metadata)\n",
    "#print(f'You have {len(data)} pages in your data')\n",
    "#print(f'There are {len(data[2].page_content)} characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fec2f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_document('files/00217 - Vivir Sin Jefe - Sergio Fernández.pdf')\n",
    "#print(data[1].page_content)\n",
    "#print(data[1].metadata)\n",
    "#print(f'You have {len(data)} pages in your data')\n",
    "#print(f'There are {len(data[2].page_content)} characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10763297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a0794476/anaconda3/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/a0794476/anaconda3/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('GPT-4', 'es')\n",
    "#print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbef9be",
   "metadata": {},
   "source": [
    "### Chunking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a26d5a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of chunks of 22\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(f'We have a total of chunks of {len(chunks)}')\n",
    "#print(chunks[1000].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e2d44",
   "metadata": {},
   "source": [
    "### Calculating Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2eba3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1152\n",
      "Embeeding cost in USD: 0.000461\n"
     ]
    }
   ],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adad85d",
   "metadata": {},
   "source": [
    "### Embedding and Uploading to a Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "776ff8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all the indexes...\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d0b44e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Index chatgpt and embeddingsOk\n"
     ]
    }
   ],
   "source": [
    "index_name = 'chatgpt'\n",
    "vector_store = insert_of_fetch_embeddings(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f9622",
   "metadata": {},
   "source": [
    "### Asking Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2097291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT is a product developed by OpenAI. It is powered by the GPT-3.5 model, which is an advanced version of GPT-3, a large language model. ChatGPT is designed to generate responses and hold conversations in a chat-like format. It is capable of understanding and generating human-like text based on the input it receives. ChatGPT has been trained on a vast amount of data and can be accessed by users through the OpenAI API or the ChatGPT Plus subscription.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is chatpGPT'\n",
    "answer = ask_and_get_answer(vector_store,q)\n",
    "#result = vector_store.similarity_search(q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbc7b569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quite or Exit to quit.\n",
      "Question #1: When was chat GPT launched?\n",
      "\n",
      "Answer: ChatGPT was launched on March 14, 2023.\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "Question #2: Until what info can we ask it?\n",
      "\n",
      "Answer: You can ask GPT-4 for information such as summarizing text, analyzing screenshots, and answering questions related to exams with diagrams. However, the extent of its capabilities to provide information may vary depending on the specific instructions given to it.\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "Question #3: Can I ask about information happened in 2022?\n",
      "\n",
      "Answer: No, I can only provide information up until the context given, which is up until the year 2020. I do not have any information about events or developments that occurred in 2022.\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "Question #4: que es github copilot?\n",
      "\n",
      "Answer: GitHub Copilot es una herramienta de programación desarrollada por GitHub y OpenAI. Es un asistente de codificación basado en IA que utiliza modelos de lenguaje de inteligencia artificial para proporcionar sugerencias de código en tiempo real mientras escribes. GitHub Copilot puede generar fragmentos de código, completar líneas de código e incluso ofrecer estructuras completas de código basadas en comentarios y descripciones de contextos dados. Utiliza el modelo GPT-4 de OpenAI junto con el conocimiento colectivo de millones de repositorios públicos en GitHub para proporcionar asistencia inteligente a los desarrolladores.\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWrite Quite or Exit to quit.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQuestion #\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     i \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1175\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[0;32m-> 1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py:1217\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Write Quite or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting... bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer = ask_and_get_answer(vector_store,q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n {\"-\"*50} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42706a4",
   "metadata": {},
   "source": [
    "### Asking with memorry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b0d4161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 se creó el 14 de marzo de 2023.\n",
      "[('Cuandos se creó GPT-4 ', 'GPT-4 se creó el 14 de marzo de 2023.')]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = 'Cuandos se creó GPT-4 '\n",
    "result, chat_history = ask_with_memory(vector_store,question,chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cada42",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Sumale a esa fecha 43 días, qué fecha es? '\n",
    "result, chat_history = ask_with_memory(vector_store,question,chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142ee65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
